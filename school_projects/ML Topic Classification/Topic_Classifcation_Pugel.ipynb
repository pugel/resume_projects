{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Classification\n",
    "#### By : Mikayla Pugel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this project I classify newsgroup posts on a variety of topics into 1 of 4 different categories. I will complete this task by training classifiers to distinguish the text based on topics inferred from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "%matplotlib inline\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below I used a dataset from the SK-learn library. I did not complete any EDA as I assumed the data was clean coming from the library. The categories are shown below as well as the separating of the data into groups that can be used in models. The data was split into test data, test label, train data, and train labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training label shape: (2034,)\n",
      "dev label shape: (676,)\n",
      "test label shape: (677,)\n",
      "labels names: ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "#load in data\n",
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "newsgroups_test  = fetch_20newsgroups(subset='test',\n",
    "                                      remove=('headers', 'footers', 'quotes'),\n",
    "                                      categories=categories)\n",
    "\n",
    "num_test = int(len(newsgroups_test.target) / 2)\n",
    "test_data, test_labels   = newsgroups_test.data[num_test:], newsgroups_test.target[num_test:]\n",
    "dev_data, dev_labels     = newsgroups_test.data[:num_test], newsgroups_test.target[:num_test]\n",
    "train_data, train_labels = newsgroups_train.data, newsgroups_train.target\n",
    "\n",
    "print('training label shape:', train_labels.shape)\n",
    "print('dev label shape:',      dev_labels.shape)\n",
    "print('test label shape:',     test_labels.shape)\n",
    "print('labels names:',         newsgroups_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I began first to just print out some examples and their corresponding label so better understand the examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:  Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych\n",
      "Train Label: 1\n",
      "---------------------------------------------------------------------\n",
      "Train Data:  \n",
      "\n",
      "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
      "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
      "folks with him, children and all, to satisfy his delusional mania. Jim\n",
      "Jones, circa 1993.\n",
      "\n",
      "\n",
      "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
      "for centuries.\n",
      "Train Label: 3\n",
      "---------------------------------------------------------------------\n",
      "Train Data:  \n",
      " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
      "\n",
      "MB>                                                             So the\n",
      "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
      "\n",
      "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
      "\n",
      "Couldn't we just say periapsis or apoapsis?\n",
      "\n",
      " \n",
      "Train Label: 2\n",
      "---------------------------------------------------------------------\n",
      "Train Data:  I have a request for those who would like to see Charley Wingate\n",
      "respond to the \"Charley Challenges\" (and judging from my e-mail, there\n",
      "appear to be quite a few of you.)  \n",
      "\n",
      "It is clear that Mr. Wingate intends to continue to post tangential or\n",
      "unrelated articles while ingoring the Challenges themselves.  Between\n",
      "the last two re-postings of the Challenges, I noted perhaps a dozen or\n",
      "more posts by Mr. Wingate, none of which answered a single Challenge.  \n",
      "\n",
      "It seems unmistakable to me that Mr. Wingate hopes that the questions\n",
      "will just go away, and he is doing his level best to change the\n",
      "subject.  Given that this seems a rather common net.theist tactic, I\n",
      "would like to suggest that we impress upon him our desire for answers,\n",
      "in the following manner:\n",
      "\n",
      "1. Ignore any future articles by Mr. Wingate that do not address the\n",
      "Challenges, until he answers them or explictly announces that he\n",
      "refuses to do so.\n",
      "\n",
      "--or--\n",
      "\n",
      "2. If you must respond to one of his articles, include within it\n",
      "something similar to the following:\n",
      "\n",
      "    \"Please answer the questions posed to you in the Charley Challenges.\"\n",
      "\n",
      "Really, I'm not looking to humiliate anyone here, I just want some\n",
      "honest answers.  You wouldn't think that honesty would be too much to\n",
      "ask from a devout Christian, would you?  \n",
      "\n",
      "Nevermind, that was a rhetorical question.\n",
      "Train Label: 0\n",
      "---------------------------------------------------------------------\n",
      "Train Data:  AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
      "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
      "\n",
      "Does anyone know more about this?  How much, to attend????\n",
      "\n",
      "Anyone want to go?\n",
      "Train Label: 2\n",
      "---------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def P1(num_examples=5):\n",
    "    \n",
    "    #print out a number of examples in an organized fashion\n",
    "    for i in range(num_examples):\n",
    "        print(\"Train Data: \", train_data[i])\n",
    "        print(\"Train Label:\", train_labels[i])\n",
    "        print(\"---------------------------------------------------------------------\")\n",
    "\n",
    "P1(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next I transofrm the data in different ways, these ways inclue unigram/bigram/word/character. I am using CountVectorizer throughout this project to turn the text into numerical features that can be run through a machine learning model. This is completed by tokening, counting, and normalizing the text. \n",
    "\n",
    "- Matrix 1: Normal vectorized matrix of the text data\n",
    "- Matrix 2: Data was vectorized only using the vocabulary given (4 topics)\n",
    "- Matrix 3: Data was vectorized by analyzing character in 2-3 length increments\n",
    "- Matrix 4: Vectorized on words that show up in more than 10 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary for matrix 1: (2034, 26879)\n",
      "The size of the vocabulary for matrix 2: (2034, 4)\n",
      "The size of the vocabulary for matrix 3: (2034, 35478)\n",
      "The size of the vocabulary for matrix 4: (2034, 3064)\n"
     ]
    }
   ],
   "source": [
    "def P2():\n",
    "    \n",
    "    #section1 - transform data into a matrix of word unigram feature vector\n",
    "    vectorizer = CountVectorizer()\n",
    "    matrix = vectorizer.fit_transform(train_data)\n",
    "      \n",
    "    #get feature names\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "# #     sorted_features = feature_names.sort()\n",
    "# #     print(sorted_features)\n",
    "#     #printing the first and last feature strings\n",
    "#     print(\"0th feature string: \" + feature_names[0])\n",
    "#     print(\"The last feature string: \" + feature_names[-1])\n",
    "    \n",
    "    shape_matrix = np.shape(matrix)\n",
    "    print(\"The size of the vocabulary for matrix 1: \" + str(np.shape(matrix)))\n",
    "    average_zeros = matrix.nnz /shape_matrix[0] #divide by vocabulary size\n",
    "#     print(\"The average number of non-zero features per example: \" + str(average_zeros))\n",
    "#     fraction_zeros = average_zeros / (shape_matrix[0]*shape_matrix[1])\n",
    "#     print(\"The fraction of non-zero entities in the matrix: \" + str(fraction_zeros))\n",
    "\n",
    "    \n",
    "    #section2 - transform data into a matrix of word unigram feature vector using own vocabulary\n",
    "    vectorizer2 = CountVectorizer(vocabulary=[\"atheism\", \"graphics\", \"space\", \"religion\"]) \n",
    "    matrix2 = vectorizer2.transform(train_data)\n",
    "#     print(np.nonzero(matrix2))\n",
    "    shape_matrix2 = np.shape(matrix2)\n",
    "    print(\"The size of the vocabulary for matrix 2: \" + str(np.shape(matrix2)))\n",
    "#     average_zeros = matrix2.nnz /shape_matrix[0] #divide by vocabulary size\n",
    "#     print(\"The average number of non-zero features per example in matrix 2: \" + str(average_zeros))\n",
    "\n",
    "\n",
    "    #section3 - transform data into a matrix of character bigram and trigram feature vectors\n",
    "    vectorizer3 = CountVectorizer(analyzer='char', ngram_range=(2,3))\n",
    "    matrix3 = vectorizer3.fit_transform(train_data)\n",
    "    print(\"The size of the vocabulary for matrix 3: \" + str(np.shape(matrix3)))\n",
    "    \n",
    "    #section4 - transform the training data into a matrix of word unigram feature \n",
    "    #vectors and prune words that appear in fewer than 10 documents\n",
    "    vectorizer4 = CountVectorizer(min_df=10)\n",
    "    matrix4 = vectorizer4.fit_transform(train_data)\n",
    "    print(\"The size of the vocabulary for matrix 4: \" + str(np.shape(matrix4)))\n",
    "    \n",
    "    #section5 - transform the training data into a matrix of word unigram feature vectors\n",
    "#     vectorizer5 = CountVectorizer() \n",
    "    \n",
    "#     #build vocabulary for both train and dev\n",
    "#     matrix5 = vectorizer5.fit_transform(train_data)\n",
    "#     matrix6 = vectorizer5.fit_transform(dev_data)\n",
    "    \n",
    "#     size_train = np.shape(matrix5)\n",
    "#     size_dev = np.shape(matrix6)\n",
    "    \n",
    "#     #compare sizes of train and dev data\n",
    "#     fraction = 1 - (size_dev[1] / size_train[1])\n",
    "#     print(\"Fraction of dev data not in training vocabulary: \" + str(fraction))\n",
    "    \n",
    "\n",
    "P2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I have trained and optimized three different models. \n",
    "- Model 1: K-Nearest Neighbors\n",
    "- Model 2: Naive-Bayes\n",
    "- Model 3: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-VALUE: 1 f1 score:  0.3805030018531525\n",
      "K-VALUE: 2 f1 score:  0.38054212404441684\n",
      "K-VALUE: 3 f1 score:  0.4084150225437623\n",
      "K-VALUE: 4 f1 score:  0.4031227993847515\n",
      "K-VALUE: 5 f1 score:  0.4287607236218357\n",
      "K-VALUE: 6 f1 score:  0.4466650540867595\n",
      "K-VALUE: 7 f1 score:  0.45047910006117586\n",
      "K-VALUE: 8 f1 score:  0.44698358117081094\n",
      "K-VALUE: 9 f1 score:  0.4365666176198027\n",
      "K-VALUE: 10 f1 score:  0.4278502905941064\n",
      "K-VALUE: 20 f1 score:  0.4243419588207676\n",
      "ALPHA: 1e-05 f1 score:  0.7533492167780379\n",
      "ALPHA: 0.0001 f1 score:  0.7628348704826354\n",
      "ALPHA: 0.001 f1 score:  0.7702518836155706\n",
      "ALPHA: 0.01 f1 score:  0.7751663218544357\n",
      "ALPHA: 0.1 f1 score:  0.7903052385098862\n",
      "ALPHA: 0.2 f1 score:  0.7876298330892171\n",
      "ALPHA: 0.5 f1 score:  0.7862862961995258\n",
      "ALPHA: 1 f1 score:  0.7777320236017224\n",
      "C: 0.1 f1 score:  0.6966243542418833\n",
      "Sum of Squared Weights:  102.26373601483208\n",
      "C: 0.2 f1 score:  0.7058733693981117\n",
      "Sum of Squared Weights:  184.49880873749936\n",
      "C: 0.3 f1 score:  0.7100848889111422\n",
      "Sum of Squared Weights:  254.82653239010708\n",
      "C: 0.4 f1 score:  0.7082817352512133\n",
      "Sum of Squared Weights:  317.19054223394846\n",
      "C: 0.5 f1 score:  0.7084739776490449\n",
      "Sum of Squared Weights:  373.7398898711874\n",
      "C: 0.6 f1 score:  0.7091615339971832\n",
      "Sum of Squared Weights:  425.7448449499764\n",
      "C: 0.7 f1 score:  0.6939142407011022\n",
      "Sum of Squared Weights:  473.8854091845524\n",
      "C: 1 f1 score:  0.6960862519544118\n",
      "Sum of Squared Weights:  601.6251828668452\n",
      "C: 5 f1 score:  0.6909245677298494\n",
      "Sum of Squared Weights:  1513.5728547158549\n",
      "C: 10 f1 score:  0.6864782927883133\n",
      "Sum of Squared Weights:  2105.3010137490087\n",
      "C: 50 f1 score:  0.6812550737668056\n",
      "Sum of Squared Weights:  4033.4297436037123\n",
      "Optimized K-Value: 7\n",
      "Optimized Alpha: 0.1\n",
      "Optimized C: 0.3\n"
     ]
    }
   ],
   "source": [
    "def P3():\n",
    "\n",
    "    k = [1,2,3,4,5,6,7,8,9,10,20]\n",
    "    a = [0.00001, 0.0001, 0.001, 0.01, 0.1, 0.2, 0.5, 1]\n",
    "    c = [0.1, 0.2, 0.3,0.4, 0.5,0.6, 0.7,1,5, 10,50]\n",
    "    \n",
    "    #set up vectorizer and transform the data\n",
    "    vectorizer = CountVectorizer()\n",
    "    matrix_train = vectorizer.fit_transform(train_data)\n",
    "    matrix_dev = vectorizer.transform(dev_data)\n",
    "    \n",
    "    #vary k values for KNN model\n",
    "    for i in k:\n",
    "        model1 = KNeighborsClassifier(n_neighbors = i)\n",
    "        model1.fit(matrix_train, train_labels)\n",
    "        predictions1 = model1.predict(matrix_dev)\n",
    "        print(\"K-VALUE:\", i, end='')\n",
    "        print(\" f1 score: \", metrics.f1_score(dev_labels, predictions1, average=\"weighted\"))\n",
    "    \n",
    "    #varying smoothing for NB model\n",
    "    for j in a:\n",
    "        model2 = MultinomialNB(alpha = j)\n",
    "        model2.fit(matrix_train, train_labels)\n",
    "        predictions2 = model2.predict(matrix_dev)\n",
    "        print(\"ALPHA:\", j, end='')\n",
    "        print(\" f1 score: \", metrics.f1_score(dev_labels, predictions2, average=\"weighted\"))\n",
    "        \n",
    "    #vary regularization strength for logistic regression\n",
    "    for l in c:\n",
    "        model3 = LogisticRegression(C=l, solver=\"liblinear\", multi_class=\"auto\")\n",
    "        model3.fit(matrix_train, train_labels)\n",
    "        predictions3 = model3.predict(matrix_dev)\n",
    "        print(\"C:\", l, end='')\n",
    "        print(\" f1 score: \", metrics.f1_score(dev_labels, predictions3, average=\"weighted\"))\n",
    "        weights = model3.coef_\n",
    "        total_sum = 0\n",
    "        for w in weights:\n",
    "            for r in w:\n",
    "                total_sum += (r**2)\n",
    "        print(\"Sum of Squared Weights: \", total_sum)\n",
    "    \n",
    "    print(\"Optimized K-Value: 7\")\n",
    "    print(\"Optimized Alpha: 0.1\")\n",
    "    print(\"Optimized C: 0.3\")\n",
    "    \n",
    "\n",
    "\n",
    "P3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now I want to analyze based on word bigrams for each topics. Below I show the top 5 feature word bigrams for each topic. I complete this by transforming the data, fitting a logisitic regression, and then sorting and indexing the feature names with their corresponding coefficient for each topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Feature', 'Atheism', 'Graphics', 'Space', 'Religion']\n",
      "['are you', 0.447, -0.2483, -0.0972, -0.3056]\n",
      "['you are', 0.4727, -0.2799, -0.4814, 0.0284]\n",
      "['was just', 0.4821, -0.1314, -0.1289, -0.2275]\n",
      "['cheers kent', 0.5557, -0.6979, -0.6638, 0.5348]\n",
      "['claim that', 0.6056, -0.199, -0.2744, -0.1404]\n",
      "['is there', -0.3409, 0.755, -0.4683, -0.2571]\n",
      "['out there', -0.2748, 0.7587, -0.479, -0.2771]\n",
      "['comp graphics', -0.2922, 0.8012, -0.3709, -0.2852]\n",
      "['in advance', -0.4594, 0.8326, -0.4385, -0.4185]\n",
      "['looking for', -0.6303, 1.1084, -0.4999, -0.5719]\n",
      "['it was', -0.203, -0.3097, 0.5254, -0.3136]\n",
      "['and such', -0.2043, -0.3375, 0.5906, -0.2182]\n",
      "['sci space', -0.258, -0.3291, 0.6211, -0.2212]\n",
      "['the moon', -0.3506, -0.4927, 0.8312, -0.2138]\n",
      "['the space', -0.2682, -0.5301, 0.8712, -0.2738]\n",
      "['of jesus', -0.0919, -0.1738, -0.2124, 0.4238]\n",
      "['but he', -0.19, -0.2169, -0.1374, 0.4919]\n",
      "['ignorance is', -0.158, -0.1711, -0.1396, 0.5043]\n",
      "['cheers kent', 0.5557, -0.6979, -0.6638, 0.5348]\n",
      "['the fbi', -0.132, -0.2113, -0.2953, 0.552]\n"
     ]
    }
   ],
   "source": [
    "def P4():\n",
    "    \n",
    "    #tranform data\n",
    "    vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "    matrix_train = vectorizer.fit_transform(train_data)\n",
    "    matrix_feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    #train model\n",
    "    model = LogisticRegression(C=0.5, solver=\"liblinear\", multi_class=\"auto\")\n",
    "    model.fit(matrix_train, train_labels)\n",
    "    \n",
    "    #gather weights and sort weights for the largest 5 \n",
    "    weights = model.coef_\n",
    "    sorted_values = np.argsort(weights, axis=1)[:,-5:]\n",
    "    \n",
    "    #find the feature name that corresponds to the largest weights\n",
    "    #do this by indexing on the feature names based on the weights index\n",
    "    feature_names =[]\n",
    "    for i in range(4):\n",
    "        for j in range(5):\n",
    "            feature_names.append(matrix_feature_names[sorted_values[i][j]])\n",
    "    feature_array = np.array(feature_names)\n",
    "    \n",
    "    #print the feature names for the matrix \n",
    "    feature_names = [\"Feature\", \"Atheism\", \"Graphics\", \"Space\", \"Religion\"]\n",
    "    print(feature_names)\n",
    "    \n",
    "    #print out the weights for each feature and topic in a matrix\n",
    "    for feature in feature_array:\n",
    "        full_array = []\n",
    "        #find index for feature name\n",
    "        index = matrix_feature_names.index(feature)\n",
    "#         print(feature, end=' ')\n",
    "        full_array.append(feature)\n",
    "        for k in range(4):\n",
    "#             if k == 3:\n",
    "#                 #print the weight in the same row as the feature and for each of the topic\n",
    "# #                   print(weights[k][index])  \n",
    "#             else:\n",
    "#                 print(weights[k][index], end=' ')\n",
    "            full_array.append(round(weights[k][index], 4))\n",
    "        print(full_array)\n",
    "    \n",
    "#     full_array = np.array(full_array)\n",
    "#     full_array = np.reshape(full_array, (5,20))\n",
    "            \n",
    "\n",
    "#     print(feature_names)\n",
    "#     for y in full_array:\n",
    "#         for z in y:\n",
    "#             print(z, end=\" \")\n",
    "\n",
    "\n",
    "P4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regularization strenght: 0.1\n",
      "F1 score l1: 0.637221813825226\n",
      "size of new_vocab: 216\n",
      "F1 score l2: 0.16520541684706788\n",
      "---------------------\n",
      "regularization strenght: 0.2\n",
      "F1 score l1: 0.6934667818378822\n",
      "size of new_vocab: 366\n",
      "F1 score l2: 0.1663883620628695\n",
      "---------------------\n",
      "regularization strenght: 0.3\n",
      "F1 score l1: 0.688753258723191\n",
      "size of new_vocab: 532\n",
      "F1 score l2: 0.17650136922656884\n",
      "---------------------\n",
      "regularization strenght: 0.4\n",
      "F1 score l1: 0.6858725408990103\n",
      "size of new_vocab: 642\n",
      "F1 score l2: 0.2002172114476057\n",
      "---------------------\n",
      "regularization strenght: 0.5\n",
      "F1 score l1: 0.6839391934405558\n",
      "size of new_vocab: 758\n",
      "F1 score l2: 0.19430789719285305\n",
      "---------------------\n",
      "regularization strenght: 0.6\n",
      "F1 score l1: 0.6891784988700599\n",
      "size of new_vocab: 915\n",
      "F1 score l2: 0.18711085815201978\n",
      "---------------------\n",
      "regularization strenght: 0.7\n",
      "F1 score l1: 0.6968210964328674\n",
      "size of new_vocab: 949\n",
      "F1 score l2: 0.19236680478727033\n",
      "---------------------\n",
      "regularization strenght: 0.8\n",
      "F1 score l1: 0.6965862876730204\n",
      "size of new_vocab: 984\n",
      "F1 score l2: 0.19827190107973633\n",
      "---------------------\n",
      "regularization strenght: 0.9\n",
      "F1 score l1: 0.6923432949021773\n",
      "size of new_vocab: 1060\n",
      "F1 score l2: 0.23005478212834\n",
      "---------------------\n",
      "regularization strenght: 1\n",
      "F1 score l1: 0.6965045182153684\n",
      "size of new_vocab: 1062\n",
      "F1 score l2: 0.23005478212834\n",
      "---------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def P6():\n",
    "    # Keep this random seed here to make comparison easier.\n",
    "#     np.random.seed(0)\n",
    "\n",
    "    \n",
    "    #train vectorizer and data\n",
    "    vectorizer = CountVectorizer()\n",
    "    trans_train = vectorizer.fit_transform(train_data)\n",
    "    trans_dev = vectorizer.transform(dev_data)\n",
    "    \n",
    "    c = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n",
    "    \n",
    "    vocab_values_graph = []\n",
    "    f1_values_graph = []\n",
    "    \n",
    "    for k in c:\n",
    "        # fit model for each regularization strength\n",
    "        print(\"regularization strenght: \" + str(k))\n",
    "        model = LogisticRegression(C=k, solver=\"liblinear\", multi_class=\"auto\", penalty=\"l1\", tol=0.015)\n",
    "        model.fit(trans_train, train_labels)\n",
    "        y_pred = model.predict(trans_dev)\n",
    "\n",
    "        #get weights and features\n",
    "        weights = model.coef_\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "#         print(np.shape(weights))\n",
    "#         print(np.shape(feature_names))\n",
    "        \n",
    "        #print f1 score\n",
    "        print(\"F1 score l1: \" + str(metrics.f1_score(dev_labels, y_pred, average=\"weighted\")))\n",
    "        \n",
    "    #     shape_matrix = np.shape(trans_train)\n",
    "    #     print(shape_matrix)\n",
    "        \n",
    "#         print(np.count_nonzero(weights, axis = 0))\n",
    "#         print(np.shape((np.count_nonzero(weights, axis = 0))))\n",
    "        \n",
    "        # find all the rows that have at least 1 nonzero weight and then append those to the new_vocab list\n",
    "        new_vocab = []\n",
    "        count = 0\n",
    "        for i in np.count_nonzero(weights, axis = 0):\n",
    "            if i > 0:\n",
    "                new_vocab.append(feature_names[count])\n",
    "                count += 1\n",
    "        \n",
    "        #print size of new vocab and append it to the list for graphing later\n",
    "        print(\"size of new_vocab: \" + str(len(new_vocab)))\n",
    "        vocab_values_graph.append(len(new_vocab))\n",
    "        \n",
    "        #tranform data based on new vocab\n",
    "        vectorizer2 = CountVectorizer(vocabulary=new_vocab)\n",
    "        trans_train2 = vectorizer2.fit_transform(train_data)\n",
    "        trans_dev2 = vectorizer2.transform(dev_data)\n",
    "        \n",
    "        #train model on new data\n",
    "        model2 = LogisticRegression(C=0.5, solver=\"liblinear\", multi_class=\"auto\", penalty=\"l2\", tol=0.015)\n",
    "        model2.fit(trans_train2, train_labels)\n",
    "        y_pred2 = model2.predict(trans_dev2)\n",
    "        \n",
    "        #print f1 score and add it to the list for graphing later\n",
    "        print(\"F1 score l2:\", metrics.f1_score(dev_labels, y_pred2, average=\"weighted\"))\n",
    "        f1_values_graph.append(metrics.f1_score(dev_labels, y_pred2, average=\"weighted\"))\n",
    "        \n",
    "        print(\"---------------------\")\n",
    "    \n",
    "    #plot the log of the vocab length values vs the f1 values from model2\n",
    "    plt.plot(np.log(vocab_values_graph), f1_values_graph)\n",
    "    plt.ylabel(\"F1 Score\")\n",
    "    plt.xlabel(\"Size of Vocab - Log\")\n",
    "    plt.title(\"Logarithmic Vocab Size and Corresponding F1 Value\")\n",
    "\n",
    "\n",
    "P6()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRry the SKywatch project in  Arizona.\n",
      "2\n",
      "---------------------------------------------------------------\n",
      "\n",
      "They can be detached in an emergency.  But expensive hardware is not thrown\n",
      "away casually (bearing in mind that nobody knew the design was defective).\n",
      "If the deployment crew had found some nasty flaw -- the lid failing to open,\n",
      "for example -- it would have been a bit embarrassing to have to throw the\n",
      "solar arrays away to get the thing back in the payload bay.\n",
      "2\n",
      "---------------------------------------------------------------\n",
      "The tongue that brings healing is a\n",
      "\ttree of life,\n",
      "\tbut a deceitful tongue crushes the\n",
      "\tspirit.\n",
      "3\n",
      "---------------------------------------------------------------\n",
      "F1 score: 0.7597662427853104\n"
     ]
    }
   ],
   "source": [
    "def P7():\n",
    "    #transform data\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    trans_train = vectorizer.fit_transform(train_data)\n",
    "    trans_dev = vectorizer.transform(dev_data)\n",
    "    \n",
    "    #fit model and find the probabilities \n",
    "    model = LogisticRegression(C=100, solver=\"liblinear\", multi_class=\"auto\", penalty=\"l2\")\n",
    "    model.fit(trans_train, train_labels)\n",
    "    y_pred = model.predict(trans_dev)\n",
    "    y_pred_probs = model.predict_proba(trans_dev)\n",
    "#     print(np.shape(y_pred_probs))\n",
    "    \n",
    "    #create a list of maximum probabilities from each row\n",
    "    count = 0\n",
    "    max_probs_per_row = []\n",
    "    for i in y_pred_probs:\n",
    "        max_prob = np.max(i)\n",
    "        max_probs_per_row.append(max_prob)\n",
    "        \n",
    "#     print(np.shape(max_probs_per_row))\n",
    "        \n",
    "    #create a list of probabilities of the correct label in each row\n",
    "    correct_label_prob = []\n",
    "    for i in range(len(y_pred_probs)):\n",
    "        correct_label_prob.append(y_pred_probs[i][dev_labels[i]])\n",
    "        \n",
    "#     print(np.shape(correct_label_prob))\n",
    "    \n",
    "    #divide the two lists created above and add them to a new list of r ratios\n",
    "    r_ratios = []\n",
    "    for i in range(len(correct_label_prob)):\n",
    "        if correct_label_prob[i] != 0:\n",
    "            r_ratios.append(max_probs_per_row[i] / correct_label_prob[i])\n",
    "        else:\n",
    "            r_ratios.append(0)\n",
    "    \n",
    "#     print(np.shape(r_ratios))\n",
    "    \n",
    "    #select top three r ratio values from r ratio list and print the corresponding data and label\n",
    "    top_three = np.argsort(r_ratios)[:3]\n",
    "    for k in top_three:\n",
    "        print(dev_data[k])\n",
    "        print(dev_labels[k])\n",
    "        print(\"---------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"F1 score: \" + str(metrics.f1_score(dev_labels, y_pred, average=\"weighted\")))\n",
    "    \n",
    "\n",
    "\n",
    "P7()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
