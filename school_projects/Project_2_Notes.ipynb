{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W205 Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The business question is to prepare the infrastructure to land the data in the correct form in order to query it. Everything I did in the command line is shown below and labeled (I completed this more for my own sake of keepings things straight in the command line). I also have some questions to explore from the queries that I will answer at the end of the report. \n",
    "\n",
    "#### Questions: \n",
    "- What are the statistics of the exams?\n",
    "- How many certifications were false and true?\n",
    "- How many exams were there and how many different exams? \n",
    "\n",
    "##### Notes:\n",
    "- I did change the data file name to \"data\" so you will see the file \"data.json\" loaded in, this is the correct one from the project 2 file.\n",
    "- For the command line history file, the project commands start at line 210 - before that was a lot of practice and does not go in a correct order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-d3494f6eed35>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-d3494f6eed35>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#download data into w205 file\n",
    "curl -L -o assessment-attempts-20180128-121051-nested.json https://goo.gl/ME6hjp\n",
    "\n",
    "#Check and if necessary, modify your docker-compose.yml file\n",
    "mkdir project_2\n",
    "#save correct YAML file into this file\n",
    "\n",
    "#move into correct file\n",
    "cd ~/w205/project2\n",
    "\n",
    "#TO BEGIN\n",
    "docker-compose up -d\n",
    "docker-compose ps\n",
    "docker ps -a\n",
    "\n",
    "#Create a symbolic link in the Spark container to the /205 mount point\n",
    "#First exec a bash shell into the spark container:\n",
    "docker-compose exec spark bash\n",
    "#Create a symbolic link from the spark directory to /w205 :\n",
    "ln -s /w205 w205\n",
    "#then exit the container\n",
    "exit\n",
    "\n",
    "####dont have to do next two lines - I did not I did it all in the command line\n",
    "#Run an enhanced version of the pyspark command line to target Jupyter Notebook\n",
    "docker-compose exec spark env PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS='notebook --no-browser --port 8888 --ip 0.0.0.0 --allow-root' pyspark\n",
    "\n",
    "#URL if planning on running pyspark in a jupyter notebook - can also just do it from the command line\n",
    "#34.123.4.189:8888 - google clous external IP address \n",
    "http://34.123.4.189:8888/?token=7520af10bd9961be5bee06e3a7eb3a135bad463f961e9b08\n",
    "\n",
    "#Let's check out hdfs before we write anything to it\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "\n",
    "#create topic - data\n",
    "docker-compose exec kafka kafka-topics --create --topic data --partitions 1 --replication-factor 1 --if-not-exists --zookeeper zookeeper:32181\n",
    "\n",
    "#use kafkacat to product messages to data topic\n",
    "docker-compose exec mids \\\n",
    "  bash -c \"cat /w205/data.json \\\n",
    "    | jq '.[]' -c \\\n",
    "    | kafkacat -P -b kafka:29092 -t data\"\n",
    "\n",
    "#spring up a pyspark process using the spark container\n",
    "docker-compose exec spark pyspark\n",
    "\n",
    "#at the pyspark prompt, read from kafka\n",
    "raw_data = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "  .option(\"subscribe\",\"data\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"endingOffsets\", \"latest\") \\\n",
    "  .load() \n",
    "\n",
    "#cache this to cut back on warnings later\n",
    "raw_data.cache()\n",
    "\n",
    "#lets explore\n",
    "raw_data.printSchema()\n",
    "\n",
    "#working on unrolling json\n",
    "data = raw_data.select(raw_data.value.cast('string'))\n",
    "\n",
    "data.select('value').take(1)\n",
    "\n",
    "data.select('value').take(1)[0].value\n",
    "\n",
    "import json\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "extracted_data = data.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "\n",
    "extracted_data.registerTempTable('data')\n",
    "\n",
    "spark.sql(\"select keen_id from data limit 20\").show()\n",
    "\n",
    "spark.sql(\"select keen_timestamp, sequences.questions[0].user_incomplete from data limit 10\").show()\n",
    "\n",
    "spark.sql(\"select exam_name, sequences.questions[0].user_incomplete from data limit 10\").show()\n",
    "\n",
    "spark.sql(\"select count(certification) from data where certification == false\").show()\n",
    "\n",
    "spark.sql(\"select count(certification) from data where certification == true\").show()\n",
    "\n",
    "spark.sql(\"select count(distinct exam_name) from data\").show()\n",
    "\n",
    "spark.sql(\"select sequences.abc123 from data limit 10\").show()\n",
    "\n",
    "def my_lambda_sequences_id(x):\n",
    "   raw_dict = json.loads(x.value)\n",
    "   my_dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"sequences_id\" : raw_dict[\"sequences\"][\"id\"]}\n",
    "   return Row(**my_dict)\n",
    "\n",
    "my_sequences = data.rdd.map(my_lambda_sequences_id).toDF()\n",
    "\n",
    "my_sequences.registerTempTable('sequences')\n",
    "\n",
    "spark.sql(\"select sequences_id from sequences limit 10\").show()\n",
    "\n",
    "spark.sql(\"select a.keen_id, a.keen_timestamp, s.sequences_id from data a join sequences s on a.keen_id = s.keen_id limit 10\").show()\n",
    "\n",
    "              \n",
    "def my_lambda_questions(x):\n",
    "   raw_dict = json.loads(x.value)\n",
    "   my_list = []\n",
    "   my_count = 0\n",
    "   for l in raw_dict[\"sequences\"][\"questions\"]:\n",
    "       my_count += 1\n",
    "       my_dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"my_count\" : my_count, \"id\" : l[\"id\"]}\n",
    "       my_list.append(Row(**my_dict))\n",
    "   return my_list\n",
    "\n",
    "\n",
    "my_questions = data.rdd.flatMap(my_lambda_questions).toDF()\n",
    "\n",
    "my_questions.registerTempTable('questions')\n",
    "\n",
    "spark.sql(\"select id, my_count from questions limit 10\").show()\n",
    "\n",
    "spark.sql(\"select q.keen_id, a.keen_timestamp, q.id from data a join questions q on a.keen_id = q.keen_id limit 10\").show()\n",
    "\n",
    "def my_lambda_correct_total(x):\n",
    "   raw_dict = json.loads(x.value)\n",
    "   my_list = []\n",
    "   if \"sequences\" in raw_dict:\n",
    "       if \"counts\" in raw_dict[\"sequences\"]:\n",
    "           if \"correct\" in raw_dict[\"sequences\"][\"counts\"] and \"total\" in raw_dict[\"sequences\"][\"counts\"]:\n",
    "               my_dict = {\"correct\": raw_dict[\"sequences\"][\"counts\"][\"correct\"],\n",
    "                          \"total\": raw_dict[\"sequences\"][\"counts\"][\"total\"]}\n",
    "               my_list.append(Row(**my_dict))\n",
    "   return my_list\n",
    "\n",
    "my_correct_total = data.rdd.flatMap(my_lambda_correct_total).toDF()\n",
    "\n",
    "my_correct_total.registerTempTable('ct')\n",
    "\n",
    "spark.sql(\"select * from ct limit 10\").show()\n",
    "\n",
    "spark.sql(\"select correct / total as score from ct limit 10\").show()\n",
    "\n",
    "spark.sql(\"select avg(correct / total)*100 as avg_score from ct limit 10\").show()\n",
    "\n",
    "spark.sql(\"select stddev(correct / total) as standard_deviation from ct limit 10\").show()\n",
    "\n",
    "#exit pyspark when completed\n",
    "#save pypark command history\n",
    "docker-compose exec spark cat /root/.python_history\n",
    "\n",
    "#check to make sure stuff is there again after writing to hdfs\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/data\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/my_questions\n",
    "docker-compose exec cloudera hadoop fs -ls /tmp/my_correct_total\n",
    "\n",
    "#Then close it down\n",
    "docker-compose down\n",
    "docker-compose ps\n",
    "\n",
    "#When absolutely finished:\n",
    "history >mpugel-history.txt\n",
    "\n",
    "#resources for unrolling Json\n",
    "#https://kontext.tech/column/spark/284/pyspark-convert-json-string-column-to-array-of-object-structtype-in-data-frame\n",
    "#https://www.tutorialspoint.com/spark_sql/spark_sql_json_datasets.htm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answering Questions Based on Queries:\n",
    "- What are the statistics of the exams?\n",
    "        - The average score was 62.65%\n",
    "        - The standard deviations was 0.311\n",
    "- How many certifications were false and true?\n",
    "        - False: 3148\n",
    "        - True: 0\n",
    "- How many exams were there and how many different exams? \n",
    "         - 3280 total exams\n",
    "         - 103 different exams\n",
    "\n",
    "#### Conclusion\n",
    "Overall, there is a lot given in this file, there is all the data about the user taking the exam, and many ids to keep track of users, questions, and exams. There is also a lot of information on questions in the exams and the users results. There were some assumptions I made about the data that I'm not sure where completely correct. I assumed the \"certifications\" section would indicate if the user \"passed\" the exam and was certified, however based on my query results I do not believe this to be the case anymore. The biggest issue with the data I found is I had no context to it, and therefore I had to assume what certain entries meant. Also, the structure of the json file was difficult to deal with and in order to get to different layers of the data I need to use functions to unroll the json file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docker compose file\n",
    "#https://docs.divio.com/en/latest/reference/docker-docker-compose/\n",
    "---\n",
    "version: '2'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 32181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    expose:\n",
    "      - \"2181\"\n",
    "      - \"2888\"\n",
    "      - \"32181\"\n",
    "      - \"3888\"\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "    expose:\n",
    "      - \"9092\"\n",
    "      - \"29092\"\n",
    "\n",
    "  cloudera:\n",
    "    image: midsw205/cdh-minimal:latest\n",
    "    expose:\n",
    "      - \"8020\" # nn\n",
    "      - \"50070\" # nn http\n",
    "\n",
    "  spark:\n",
    "    image: midsw205/spark-python:0.0.5\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n",
    "    command: bash\n",
    "    depends_on:\n",
    "      - cloudera\n",
    "    environment:\n",
    "      HADOOP_NAMENODE: cloudera\n",
    "    expose:\n",
    "      - \"8888\"\n",
    "    ports:\n",
    "      - \"8888:8888\"\n",
    "\n",
    "  mids:\n",
    "    image: midsw205/base:latest\n",
    "    stdin_open: true\n",
    "    tty: true\n",
    "    volumes:\n",
    "      - ~/w205:/w205\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### history from pyspark - copied from command line after running docker-compose exec spark cat /root/.python_history\n",
    "raw_data = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:29092\") \\\n",
    "  .option(\"subscribe\",\"data\") \\\n",
    "  .option(\"startingOffsets\", \"earliest\") \\\n",
    "  .option(\"endingOffsets\", \"latest\") \\\n",
    "  .load() \n",
    "raw_data.cache()\n",
    "raw_data.printSchema()\n",
    "data = raw_data.select(raw_data.value.cast('string'))\n",
    "data.select('value').take(1)\n",
    "data.select('value').take(1)[0].value\n",
    "import json\n",
    "from pyspark.sql import Row\n",
    "extracted_data = data.rdd.map(lambda x: Row(**json.loads(x.value))).toDF()\n",
    "extracted_data.registerTempTable('data')\n",
    "spark.sql(\"select keen_id from data limit 20\").show()\n",
    "spark.sql(\"select keen_timestamp, sequences.questions[0].user_incomplete from data limit 10\").show()\n",
    "spark.sql(\"select exam_name, sequences.questions[0].user_incomplete from data limit 10\").show()\n",
    "spark.sql(\"select sequences.abc123 from data limit 10\").show()\n",
    "spark.sql(\"select count(certification) from data where certification == False\").show()\n",
    "spark.sql(\"select count(certification) from data where certification == True\").show()\n",
    "spark.sql(\"select count(certification) from data where certification == false\").show()\n",
    "spark.sql(\"select count(certification) from data where certification == true\").show()\n",
    "spark.sql(\"select count(unique exam_name) from data\").show()\n",
    "spark.sql(\"select count(exam_name) from data\").show()\n",
    "spark.sql(\"select count(unique(exam_name)) from data\").show()\n",
    "spark.sql(\"select count(distinct exam_name) from data\").show()\n",
    "def my_lambda_questions(x):\n",
    "   raw_dict = json.loads(x.value)\n",
    "   my_list = []\n",
    "   my_count = 0\n",
    "   for l in raw_dict[\"sequences\"][\"questions\"]:\n",
    "       my_count += 1\n",
    "       my_dict = {\"keen_id\" : raw_dict[\"keen_id\"], \"my_count\" : my_count, \"id\" : l[\"id\"]}\n",
    "       my_list.append(Row(**my_dict))\n",
    "   return my_list\n",
    "my_questions = data.rdd.flatMap(my_lambda_questions).toDF()\n",
    "my_questions.registerTempTable('questions')\n",
    "spark.sql(\"select id, my_count from questions limit 10\").show()\n",
    "spark.sql(\"select q.keen_id, a.keen_timestamp, q.id from data a join questions q on a.keen_id = q.keen_id limit 10\").show()\n",
    "def my_lambda_correct_total(x):\n",
    "   raw_dict = json.loads(x.value)\n",
    "   my_list = []\n",
    "   if \"sequences\" in raw_dict:\n",
    "       if \"counts\" in raw_dict[\"sequences\"]:\n",
    "           if \"correct\" in raw_dict[\"sequences\"][\"counts\"] and \"total\" in raw_dict[\"sequences\"][\"counts\"]:\n",
    "               my_dict = {\"correct\": raw_dict[\"sequences\"][\"counts\"][\"correct\"],\n",
    "                          \"total\": raw_dict[\"sequences\"][\"counts\"][\"total\"]}\n",
    "               my_list.append(Row(**my_dict))\n",
    "   return my_list\n",
    "my_correct_total = data.rdd.flatMap(my_lambda_correct_total).toDF()\n",
    "my_correct_total.registerTempTable('ct')\n",
    "spark.sql(\"select * from ct limit 10\").show()\n",
    "spark.sql(\"select correct / total as score from ct limit 10\").show()\n",
    "spark.sql(\"select avg(correct / total)*100 as avg_score from ct limit 10\").show()\n",
    "spark.sql(\"select stddev(correct / total) as standard_deviation from ct limit 10\").show()\n",
    "exit\n",
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
